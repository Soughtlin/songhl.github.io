#### Sun Yat-sen University (SYSU), China

**Aug. 2024 – Dec. 2024**
- Responsible for coding and main sections of paper writing

**Brief introduction:**
This research focuses on improving the understanding capabilities of multimodal video-language large models (VLLMs) using adaptive sparse memory for long videos.
- Reproduced the baseline model MA-LLM (a VLLM based on BLIP2).
- Under the guidance of Jisheng Dang (PhD student at SYSU), improved the memory bank compression strategy of MA-LLM and incorporated a cross-attention module to enhance the interaction between vision and language.
- Experimental results demonstrated significant improvements over the baseline in various video-question answering scenarios.
- The paper has been accepted by ICME 2025.

#### Sun Yat-sen University, China

**Dec. 2024 – Mar. 2025**
- Responsible for conception, algorithm design, coding, and manuscript writing

**Brief introduction:**
This research focuses on addressing single-frame bias in long videos and aims to reduce hallucinations in VLLMs for long video temporal understanding tasks, while improving model performance and computational efficiency.
- Reproduced the baseline models MA-LLM and ATP.
- Identified and validated the existence of single-frame static bias in long video understanding tasks, and proposed improvements to the ATP module by adding the Discriminator and the Selector to address this bias, leading to the design of the DTP module.
- Modified the MA-LLM Memory Bank module to enable the discrimination of static bias and dynamic selection based on it, and incorporated a CLIP-based contrastive learning module to enhance visual-text alignment.
- Experimental results demonstrated improvements over the baseline in various video question answering and video captioning tasks, with significantly reduced computation time.
- The paper has been submitted to IJCAI 2025 (under review).

#### Sun Yat-sen University, National University of Singapore (NUS)

**Mar. 2025 – May 2025**
- Responsible for core innovation discovery, coding, and paper writing

**Brief introduction:**
This research focuses on Trustworthy Multimodal LLMs, with a specific goal of moment retrieval, improving the model’s reliability and temporal grounding capabilities.
- Designed three complementary reasoning paths and a reflection agent (PoE + MoE fusion) that jointly boost grounding fidelity \emph{without} sacrificing answer accuracy.
- Achieved new state-of-the-art on NExT-GQA and DeVE-QA. Remarkably, the 2B variant already surpasses all existing 7B baselines.
- Supervised by [Junbin Xiao](https://doc-doc.github.io/cv/#) (NUS) and Jisheng Dang (SYSU). The paper has been submitted to NeurIPS 2025 and is currently under review.
