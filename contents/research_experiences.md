#### Sun Yat-sen University (SYSU), China

**Aug. 2024 – Nov. 2024**
- Responsible for coding and main sections of paper writing
- GPA: 3.9/4.0

**Brief introduction:**
This research focuses on improving the understanding capabilities of multimodal video-language large models (VLLMs) using adaptive sparse memory for long videos.
- Reproduced the baseline model MA-LLM (a VLLM based on BLIP2).
- Under the guidance of Jisheng Dang (PhD student at SYSU), improved the memory bank compression strategy of MA-LLM and incorporated a cross-attention module to enhance the interaction between vision and language.
- Experimental results demonstrated significant improvements over the baseline in various video-question answering scenarios.
- The paper has been accepted by ICME 2025.

#### Sun Yat-sen University, China

**Nov. 2024 – Mar. 2025**
- Responsible for discussion and determination of core innovations, coding, and main sections of paper writing

**Brief introduction:**
This research focuses on addressing single-frame bias in long videos and aims to reduce hallucinations in VLLMs for long video temporal understanding tasks, while improving model performance and computational efficiency.
- Reproduced the baseline models MA-LLM and ATP.
- Identified and validated the existence of single-frame static bias in long video understanding tasks, and proposed improvements to the ATP module by adding the Discriminator and the Selector to address this bias, leading to the design of the DTP module.
- Modified the MA-LLM Memory Bank module to enable the discrimination of static bias and dynamic selection based on it, and incorporated a CLIP-based contrastive learning module to enhance visual-text alignment.
- Experimental results demonstrated improvements over the baseline in various video question answering and video captioning tasks, with significantly reduced computation time.
- The paper has been submitted to IJCAI 2025 (under review).

#### Sun Yat-sen University, National University of Singapore (NUS)

**Mar. 2025 – Present**
- Responsible for core innovation discovery, coding, and paper writing

**Brief introduction:**
This research focuses on Trustworthy Multimodal LLMs, with a specific goal of moment retrieval, improving the model’s reliability and temporal grounding capabilities.
- Under the guidance of Junbin Xiao (Research Fellow at NUS), exploring the baseline model and core innovations, determining the goal to enhance model performance on the NExT-GQA dataset and improving video grounding capabilities.
- Reproduced the current state-of-the-art model Chrono.
- The work is still ongoing.
